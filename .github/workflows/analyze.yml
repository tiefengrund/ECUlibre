# File: .github/workflows/map-visualize.yml
name: ECU Map Visualizer
run-name: ECU Map Visualizer • ${{ github.event_name }} • ${{ github.ref_name }} • ${{ github.run_id }}

on:
  push:
    branches: ['**']
  workflow_dispatch:
    inputs:
      bins:
        description: "Glob für .bin-Dateien"
        type: string
        default: "rawdata/**/*.bin"
        required: false
      specs:
        description: "Glob für Map-Spezifikationen (YAML)"
        type: string
        default: "mapspecs/**/*.y?(a)ml"
        required: false
      deepseek:
        description: "Glob für DeepSeek-Map-JSON (optional)"
        type: string
        default: "deepseek/maps/**/*.json"
        required: false
      outdir:
        description: "Output-Verzeichnis"
        type: string
        default: "out/mapviz"
        required: false
      commit_results:
        description: "Ergebnisse nach docs/mapviz committen"
        type: boolean
        default: false
        required: false

permissions:
  contents: write
  actions: read

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  visualize:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with: { fetch-depth: 0 }

      - name: Setup Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Install Python deps
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install numpy pyyaml jsonschema matplotlib==3.* pillow

      - name: Resolve inputs (defaults for push)
        shell: bash
        run: |
          set -euo pipefail
          # Bei workflow_dispatch sind inputs.* definiert, sonst nicht – daher Fallbacks hier:
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            BINS="${{ inputs.bins }}"
            SPECS="${{ inputs.specs }}"
            DEEPSEEK="${{ inputs.deepseek }}"
            OUTDIR="${{ inputs.outdir }}"
            COMMIT="${{ inputs.commit_results }}"
          else
            BINS="rawdata/**/*.bin"
            SPECS="mapspecs/**/*.y?(a)ml"
            DEEPSEEK="deepseek/maps/**/*.json"
            OUTDIR="out/mapviz"
            COMMIT="false"
          fi
          echo "BINS=$BINS" >> "$GITHUB_ENV"
          echo "SPECS=$SPECS" >> "$GITHUB_ENV"
          echo "DEEPSEEK=$DEEPSEEK" >> "$GITHUB_ENV"
          echo "OUTDIR=$OUTDIR" >> "$GITHUB_ENV"
          echo "COMMIT=$COMMIT" >> "$GITHUB_ENV"

      - name: Beispiel-Mapspezifikation anlegen (falls keine vorhanden)
        shell: bash
        run: |
          set -euo pipefail
          shopt -s nullglob
          FOUND=(mapspecs/*.yml mapspecs/*.yaml)
          if [[ ${#FOUND[@]} -eq 0 ]]; then
            python - <<'PY'
          import os, yaml
          os.makedirs("mapspecs", exist_ok=True)
          doc = {
            "schema_version": "1.0",
            "dataset_hint": {"brand":"Generic","model":"Example","notes":"Sample spec; set real offsets/sizes for your ECU."},
            "maps": [{
              "name":"Torque_Limit","offset":"0x001000","rows":16,"cols":16,
              "dtype":"u16","endian":"little","scale":0.1,"add":0.0,
              "x_axis":{"start":0,"step":250,"count":16,"unit":"rpm"},
              "y_axis":{"start":0,"step":10,"count":16,"unit":"load"}
            }]
          }
with open("mapspecs/example_torque.yml","w",encoding="utf-8") as f:
  yaml.safe_dump(doc, f, sort_keys=False)
print("Wrote mapspecs/example_torque.yml")
PY
    fi


      - name: mapviz.py (self-contained Reporter & Visualizer)
        shell: bash
        run: |
          set -euo pipefail
          mkdir -p tools
          cat > tools/mapviz.py <<'PY'
#!/usr/bin/env python3
import argparse, json, os, sys, glob, hashlib, math, pathlib, re
from typing import Dict, Any, List
import numpy as np
import yaml
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

DTYPES = {"u8": np.uint8, "s8": np.int8, "u16": np.uint16, "s16": np.int16,
          "u32": np.uint32, "s32": np.int32, "f32": np.float32}
ENDIANS = {"little": "<", "big": ">"}
def to_int(x): return int(x,16) if isinstance(x,str) and x.lower().startswith("0x") else int(x)
def sha256_path(p):
    h=hashlib.sha256()
    with open(p,"rb") as f:
        for chunk in iter(lambda: f.read(1<<20), b""):
            h.update(chunk)
    return h.hexdigest()
def shannon_entropy(data: bytes) -> float:
    if not data: return 0.0
    from collections import Counter
    c = Counter(data); n = len(data)
    return -sum((cnt/n)*math.log2(cnt/n) for cnt in c.values())
def find_strings(data: bytes, minlen=6, limit=40):
    out=[]; cur=[]
    for b in data:
        if 32 <= b <= 126: cur.append(chr(b))
        else:
            if len(cur) >= minlen:
                out.append("".join(cur))
                if len(out) >= limit: break
            cur=[]
    if len(cur) >= minlen and len(out) < limit: out.append("".join(cur))
    return out
def byte_histogram(data: bytes, png: str):
    arr = np.frombuffer(data, dtype=np.uint8)
    hist, _ = np.histogram(arr, bins=256, range=(0,256))
    fig = plt.figure(figsize=(10,4)); ax = fig.add_subplot(111)
    ax.bar(np.arange(256), hist, width=1.0)
    ax.set_title("Byte histogram"); ax.set_xlabel("byte"); ax.set_ylabel("count")
    fig.tight_layout(); fig.savefig(png, dpi=150); plt.close(fig)
def load_specs(patterns: List[str]):
    files=[]; [files.extend(glob.glob(pat, recursive=True)) for pat in patterns]
    specs=[]
    for p in files:
        try:
            with open(p,"r",encoding="utf-8") as f: specs.append(yaml.safe_load(f))
        except Exception as e: print(f"[spec] skip {p}: {e}", file=sys.stderr)
    return specs
def index_deepseek(patterns: List[str]):
    files=[]; [files.extend(glob.glob(pat, recursive=True)) for pat in patterns]
    idx={}
    for p in files:
        try:
            with open(p,"r",encoding="utf-8") as f: obj=json.load(f)
            maps=[]
            if isinstance(obj, dict) and "maps" in obj: maps = obj["maps"]
            elif isinstance(obj, dict) and ("values" in obj or "data" in obj): maps=[obj]
            elif isinstance(obj, list): maps = obj
            for m in maps:
                name = str(m.get("name") or m.get("map") or pathlib.Path(p).stem)
                vals = m.get("values") or m.get("data")
                rows = m.get("rows") or m.get("height"); cols = m.get("cols") or m.get("width")
                if vals is None: continue
                arr = np.array(vals, dtype=np.float64)
                if rows and cols:
                    try: arr = arr.reshape((int(rows), int(cols)))
                    except Exception: pass
                idx[name] = {"array": arr, "source": p}
        except Exception as e: print(f"[deepseek] skip {p}: {e}", file=sys.stderr)
    return idx
def mesh_axes(spec_map, rows, cols):
    def axbuild(axspec, count):
        if not axspec: return np.arange(count, dtype=np.float64)
        if "values" in axspec:
            vals = np.array(axspec["values"], dtype=np.float64)
            if len(vals) != count: vals = np.resize(vals, (count,))
            return vals
        start = float(axspec.get("start", 0.0)); step = float(axspec.get("step", 1.0))
        return start + step*np.arange(count, dtype=np.float64)
    X = axbuild(spec_map.get("x_axis"), cols); Y = axbuild(spec_map.get("y_axis"), rows)
    return np.meshgrid(X, Y)
def read_map_from_bin(bin_path, m):
    off = to_int(m["offset"]); rows = int(m["rows"]); cols = int(m["cols"])
    dtype = m.get("dtype","u16"); endian=m.get("endian","little")
    scale=float(m.get("scale",1.0)); add=float(m.get("add",0.0))
    if dtype not in DTYPES or endian not in ENDIANS: raise ValueError("bad dtype/endian")
    bsize = np.dtype(DTYPES[dtype]).itemsize; need = rows*cols*bsize
    with open(bin_path, "rb") as f:
        f.seek(off); buf=f.read(need)
        if len(buf)<need: raise ValueError(f"Not enough bytes at 0x{off:X} need {need} got {len(buf)}")
        arr = np.frombuffer(buf, dtype=np.dtype(ENDIANS[endian]+DTYPES[dtype].name))
        arr = arr.reshape((rows, cols)).astype(np.float64)
        return arr*scale + add
def save_csv(path, X, Y, Z):
    with open(path,"w",encoding="utf-8") as f:
        f.write("y\\x," + ",".join(map(str, X[0].tolist())) + "\n")
        for r in range(Z.shape[0]):
            f.write(str(Y[r,0]) + "," + ",".join(f"{v:.6g}" for v in Z[r,:]) + "\n")
def surface_pair(outpng, title, X, Y, Zbin, Zds=None):
    if Zds is not None and Zds.shape != Zbin.shape: Zds=None
    if Zds is None:
        fig = plt.figure(figsize=(9,7)); ax = fig.add_subplot(111, projection="3d")
        ax.plot_surface(X, Y, Zbin, cmap="viridis", linewidth=0, antialiased=True)
        ax.set_title(title + " (BIN)")
    else:
        fig = plt.figure(figsize=(16,7))
        ax1 = fig.add_subplot(121, projection="3d"); ax2 = fig.add_subplot(122, projection="3d")
        ax1.plot_surface(X, Y, Zbin, cmap="viridis", linewidth=0, antialiased=True); ax1.set_title(title + " (BIN)")
        ax2.plot_surface(X, Y, Zds, cmap="plasma", linewidth=0, antialiased=True); ax2.set_title(title + " (DeepSeek)")
    for ax in fig.axes:
        try: ax.set_xlabel("X"); ax.set_ylabel("Y"); ax.set_zlabel("Z")
        except Exception: pass
    fig.tight_layout(); fig.savefig(outpng, dpi=200); plt.close(fig)
def surface_diff(outpng, title, X, Y, Zbin, Zds):
    D = Zds - Zbin
    fig = plt.figure(figsize=(8,7)); ax = fig.add_subplot(111, projection="3d")
    ax.plot_surface(X, Y, D, cmap="coolwarm", linewidth=0, antialiased=True)
    ax.set_title(title + " (DeepSeek - BIN)")
    ax.set_xlabel("X"); ax.set_ylabel("Y"); ax.set_zlabel("ΔZ")
    fig.tight_layout(); fig.savefig(outpng, dpi=200); plt.close(fig)
def read_metadata_for_bin(bin_path):
    p = pathlib.Path(bin_path).resolve()
    for parent in [p.parent] + list(p.parents):
        cand = parent / "metadata.yml"
        if cand.exists():
            try:
                with open(cand,"r",encoding="utf-8") as f: y = yaml.safe_load(f) or {}
                keep = {k:str(v) for k,v in y.items() if k in ("brand","model","generation","ecu_vendor","ecu_model","firmware","region","schema_version")}
                return keep
            except Exception: pass
    return {}
def analyze_bin(bin_path, outdir):
    dat = pathlib.Path(bin_path).read_bytes()
    info = {"path": str(bin_path), "size_bytes": len(dat),
            "sha256": sha256_path(bin_path),
            "entropy_bits_per_byte": round(shannon_entropy(dat), 4),
            "pct_zero": round(100.0*dat.count(0)/max(1,len(dat)), 3)}
    png_hist = os.path.join(outdir, "histogram.png"); byte_histogram(dat, png_hist)
    strings = find_strings(dat, minlen=6, limit=40)
    return info, strings, os.path.relpath(png_hist)
def render_report(bin_path, out_dir, maps, ds_idx):
    os.makedirs(out_dir, exist_ok=True)
    info, strings, hist_rel = analyze_bin(bin_path, out_dir)
    md = [f"# Report for `{os.path.basename(bin_path)}`", "", "## File Info", ""]
    meta = read_metadata_for_bin(bin_path)
    if meta: md.append("**Metadata (from metadata.yml):**  " + ", ".join([f"{k}={v}" for k,v in meta.items()]))
    md += [f"- Path: `{info['path']}`",
           f"- Size: `{info['size_bytes']}` bytes",
           f"- SHA256: `{info['sha256']}`",
           f"- Shannon entropy: `{info['entropy_bits_per_byte']}` bits/byte",
           f"- Zero bytes: `{info['pct_zero']}%`", "",
           "### Byte histogram", f"![histogram]({hist_rel})", ""]
    if strings:
        md.append("### Top printable strings (first 40)")
        for s in strings: md.append(f"- `{s.replace('|','\\|')}`")
        md.append("")
    if not maps:
        md.append("> No maps found from specs.")
    else:
        md.append("## Maps")
        for m in maps:
            name = str(m.get("name","<unnamed>")); safe = re.sub(r'[^a-zA-Z0-9_.-]+', '_', name)
            try: Zbin = read_map_from_bin(bin_path, m)
            except Exception as e:
                md.append(f"### {name}\n- ⚠️ {e}\n"); continue
            rows, cols = Zbin.shape; X, Y = np.meshgrid(
                np.array(m.get("x_axis",{}).get("values") or [m.get("x_axis",{}).get("start",0)+m.get("x_axis",{}).get("step",1)*i for i in range(cols)], dtype=float),
                np.array(m.get("y_axis",{}).get("values") or [m.get("y_axis",{}).get("start",0)+m.get("y_axis",{}).get("step",1)*i for i in range(rows)], dtype=float)
            )
            Zds=None
            if name in ds_idx:
                try:
                    z = np.array(ds_idx[name]["array"], dtype=float)
                    if z.shape == Zbin.shape: Zds = z
                except Exception: pass
            png_pair = os.path.join(out_dir, f"{safe}.pair.png"); surface_pair(png_pair, name, X, Y, Zbin, Zds)
            csv_path = os.path.join(out_dir, f"{safe}.csv"); save_csv(csv_path, X, Y, Zbin)
            md += [f"### {name}", "",
                   f"**Offset:** `0x{to_int(m['offset']):X}`, **shape:** `{rows}×{cols}`, **dtype:** `{m.get('dtype','u16')}`, **endian:** `{m.get('endian','little')}`  ",
                   f"[CSV]({os.path.relpath(csv_path)})  ",
                   f"![{name}]({os.path.relpath(png_pair)})", ""]
            if Zds is not None:
                png_diff = os.path.join(out_dir, f"{safe}.diff.png"); surface_diff(png_diff, name, X, Y, Zbin, Zds)
                md += [f"**Diff (DeepSeek − BIN):**", f"![{name} diff]({os.path.relpath(png_diff)})", ""]
    rep = os.path.join(out_dir, "REPORT.md")
    with open(rep, "w", encoding="utf-8") as f: f.write("\n".join(md) + "\n")
    return rep
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--bins", default="rawdata/**/*.bin")
    ap.add_argument("--specs", default="mapspecs/**/*.y?(a)ml")
    ap.add_argument("--deepseek", default="deepseek/maps/**/*.json")
    ap.add_argument("--outdir", default="out/mapviz")
    args = ap.parse_args()
    bin_paths = glob.glob(args.bins, recursive=True)
    spec_objs = load_specs([args.specs]); ds_index = index_deepseek([args.deepseek])
    all_maps=[m for spec in spec_objs for m in (spec.get("maps") or [])]
    pathlib.Path(args.outdir).mkdir(parents=True, exist_ok=True)
    index_lines = ["# Map Visualization Index", ""]
    if not bin_paths: print(f"[INFO] no .bin matched: {args.bins}", file=sys.stderr)
    for binp in bin_paths:
        base = pathlib.Path(binp).name
        dst = pathlib.Path(args.outdir) / pathlib.Path(base).with_suffix("")
        dst.mkdir(parents=True, exist_ok=True)
        rep = render_report(binp, str(dst), all_maps, ds_index)
        index_lines.append(f"- [{base}]({os.path.relpath(rep, args.outdir)})")
    with open(os.path.join(args.outdir, "INDEX.md"), "w", encoding="utf-8") as f:
        f.write("\n".join(index_lines) + "\n")
if __name__ == "__main__": main()
PY
          chmod +x tools/mapviz.py

      - name: Visualize ALL bins in rawdata
        run: |
          python tools/mapviz.py \
            --bins     "${BINS}" \
            --specs    "${SPECS}" \
            --deepseek "${DEEPSEEK}" \
            --outdir   "${OUTDIR}"

      - name: Upload artifacts (reports + figures)
        uses: actions/upload-artifact@v4
        with:
          name: mapviz-output
          path: ${{ env.OUTDIR }}

      - name: Optional commit to docs/mapviz
        if: ${{ env.COMMIT == 'true' }}
        run: |
          set -euo pipefail
          mkdir -p docs/mapviz
          rsync -a "${OUTDIR}/" docs/mapviz/
          git config user.name  "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add docs/mapviz
          git commit -m "docs(mapviz): add/update reports (auto)" || echo "Nothing to commit"
          git push
